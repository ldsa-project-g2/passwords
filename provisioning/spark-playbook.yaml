---
- name: Housekeeping
  hosts: all

  tasks:
    - name: Install languages
      become: true
      apt:
        name: language-pack-en
        state: present

    - name: Setup UK locale
      become: true
      locale_gen:
        name: en_GB.UTF-8
        state: present

    - name: Update host file
      become: yes
      lineinfile:
        path: /etc/hosts
        regexp: '^127\.0\.0\.1'
        line: '127.0.0.1 localhost {{ansible_hostname}}'
        owner: root
        group: root
        mode: 0644

    - name: Install packages
      become: true
      apt: name={{ item }} state=present
      with_items:
        - python3
        - python-sh
        - default-jdk
        - htop
        - scala
        - nfs-common
        - python3-pip

    - name: Create spark install directory
      become: yes
      file:
        path: /usr/local/spark
        state: directory
        owner: ubuntu
        group: ubuntu
        mode: "u=rwx,g=wx,o=xr"

    - name: Set authorized key took from file
      authorized_key:
        user: ubuntu
        state: present
        key: "{{ lookup('file', 'master-keys/id_rsa.pub') }}"

    - name: Fetch and unzip Spark
      become: yes
      unarchive:
        src: http://apache.mirrors.spacedump.net/spark/spark-2.3.0/spark-2.3.0-bin-hadoop2.7.tgz
        dest: /usr/local/spark/
        creates: /usr/local/spark/spark-2.3.0-bin-hadoop2.7/
        owner: ubuntu
        group: ubuntu
        mode: "u=rwx,g=wx,o=xr"
        remote_src: yes


    - name: Set SPARK_HOME and JAVA_HOME
      become: yes
      copy:
        content: |
          export SPARK_HOME=/usr/local/spark/spark-2.3.0-bin-hadoop2.7
          export JAVA_HOME=/usr/lib/jvm/java-1.8.0-openjdk-amd64

        dest: "/etc/profile.d/sparkenv.sh"
        mode: "u=rwx,g=wx,o=xr"


    - name: Add spark-worker.service
      become: true
      systemdunit:
        name: "spark-worker.service"
        state: present
        content: |
          [Unit]
          Description=Apache Spark Worker Service
          After=network.target
          After=systemd-user-sessions.service
          After=network-online.target

          [Service]
          User=ubuntu
          Type=forking
          ExecStart=/usr/local/spark/spark-2.3.0-bin-hadoop2.7/sbin/start-slave.sh spark://192.168.1.86:7077
          ExecStop=/usr/local/spark/spark-2.3.0-bin-hadoop2.7/sbin/stop-slaves.sh
          TimeoutSec=30
          Restart= on-failure
          RestartSec= 30
          StartLimitInterval=350
          StartLimitBurst=10

          [Install]
          WantedBy=multi-user.target

    - name: Enable the Spark Worker
      become: true
      systemd:
        name: "spark-worker.service"
        state: stopped
        enabled: false

    #- name: Ensure data directory exists
    #  file:
    #    path: /home/ubuntu/data
    #    state: directory
    #    owner: ubuntu
    #    group: ubuntu
    # mode: "u=rwx,g=rwx,o=xr"

    - name: Setup the Spark environment file
      copy:
        content: |
          export SPARK_MASTER_HOST=192.168.1.127
          export SPARK_MASTER_PORT=7077
          export PYSPARK_PYTHON=python3

        dest: "/usr/local/spark/spark-2.3.0-bin-hadoop2.7/conf/spark-env.sh"

    - name: Mount NFS share from the file server
      become: yes
      mount:
        path: /home/ubuntu/data
        src: 192.168.1.13:/home/ubuntu/data
        fstype: nfs
        opts: ro,auto,nofail,noatime,nolock,intr,tcp,actimeo=1800
        state: mounted

    - name: Upgrade pip
      become: yes
      pip:
        name: pip
        state: latest
        executable: pip3

    - name: Copy Python requirements
      copy:
        src: ../requirements.txt
        dest: /home/ubuntu/requirements.txt

    - name: Install Python requirements
      become: yes
      pip:
        requirements: /home/ubuntu/requirements.txt


- name: Master setup
  hosts: master
  tasks:
    - name: ensure private key and public one are present
      copy:
        src: master-keys/
        dest: "/home/ubuntu/.ssh/"
        mode: 0600

    - name: Install a list of spark workers in the Master
      copy:
        content: |
          localhost
          192.168.1.138
          192.168.1.134
          192.168.1.136
          192.168.1.124
          192.168.1.133
          192.168.1.131
          192.168.1.132
          192.168.1.122
          192.168.1.89
          192.168.1.128

        dest: "/usr/local/spark/spark-2.3.0-bin-hadoop2.7/conf/slaves"

    - name: Add spark-master.service
      become: true
      systemdunit:
        name: "spark-master.service"
        state: present
        content: |
          [Unit]
          Description=Apache Spark Master Service
          After=network.target
          After=systemd-user-sessions.service
          After=network-online.target

          [Service]
          User=ubuntu
          Type=forking
          ExecStart=/usr/local/spark/spark-2.3.0-bin-hadoop2.7/sbin/start-all.sh
          ExecStop=/usr/local/spark/spark-2.3.0-bin-hadoop2.7/sbin/stop-all.sh
          TimeoutSec=30
          Restart= on-failure
          RestartSec= 30
          StartLimitInterval=350
          StartLimitBurst=10

          [Install]
          WantedBy=multi-user.target

    - name: Enable the Spark Master
      become: true
      systemd:
        name: "spark-master.service"
        state: stopped
        enabled: false

#    - name: Copy the code
#      copy:
#        src: ../
#        dest: /home/ubuntu/
